{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 3: Tesla K80 (CNMeM is disabled)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU, ParametricSoftplus\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 3, 32, 32)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "nb_classes = 10\n",
    "nb_epoch = 100\n",
    "\n",
    "img_channels = 3\n",
    "img_rows = 32\n",
    "img_cols = 32\n",
    "\n",
    "# the data, shuffled and split between tran and test sets\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0...1...2...3...4...5...6...7...8...9...10...11...12...13...14...15...16...17...18...19...20...21...22...23...24...X_train shape: (100000, 3, 32, 32)\n",
      "100000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=None,\n",
    "    width_shift_range=None,\n",
    "    height_shift_range=None,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False)\n",
    "\n",
    "batch = 0\n",
    "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=2048):\n",
    "    print(batch, end='...', flush=True)\n",
    "    X_train = np.vstack((X_train, X_batch))\n",
    "    y_train = np.vstack((y_train, y_batch))\n",
    "    batch += 1\n",
    "    \n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train shape: (100000, 10)\n",
      "Y_test shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print('Y_test shape:', Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "100000/100000 [==============================] - 55s - loss: 1.2535 - acc: 0.5512 - val_loss: 0.7868 - val_acc: 0.7219\n",
      "Epoch 00000: val_loss improved from inf to 0.78679, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 2/100\n",
      "100000/100000 [==============================] - 54s - loss: 0.8126 - acc: 0.7169 - val_loss: 0.6505 - val_acc: 0.7741\n",
      "Epoch 00001: val_loss improved from 0.78679 to 0.65051, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 3/100\n",
      "100000/100000 [==============================] - 54s - loss: 0.6964 - acc: 0.7586 - val_loss: 0.5674 - val_acc: 0.8044\n",
      "Epoch 00002: val_loss improved from 0.65051 to 0.56742, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 4/100\n",
      "100000/100000 [==============================] - 54s - loss: 0.6299 - acc: 0.7814 - val_loss: 0.5577 - val_acc: 0.8079\n",
      "Epoch 00003: val_loss improved from 0.56742 to 0.55768, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 5/100\n",
      "100000/100000 [==============================] - 54s - loss: 0.5878 - acc: 0.7951 - val_loss: 0.5649 - val_acc: 0.8123\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 6/100\n",
      "100000/100000 [==============================] - 54s - loss: 0.5578 - acc: 0.8086 - val_loss: 0.5003 - val_acc: 0.8311\n",
      "Epoch 00005: val_loss improved from 0.55768 to 0.50028, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 7/100\n",
      "100000/100000 [==============================] - 54s - loss: 0.5376 - acc: 0.8139 - val_loss: 0.4991 - val_acc: 0.8314\n",
      "Epoch 00006: val_loss improved from 0.50028 to 0.49913, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 8/100\n",
      "100000/100000 [==============================] - 54s - loss: 0.5176 - acc: 0.8216 - val_loss: 0.5188 - val_acc: 0.8231\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 9/100\n",
      "100000/100000 [==============================] - 54s - loss: 0.5003 - acc: 0.8261 - val_loss: 0.5392 - val_acc: 0.8173\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 10/100\n",
      "100000/100000 [==============================] - 54s - loss: 0.4860 - acc: 0.8317 - val_loss: 0.4797 - val_acc: 0.8392\n",
      "Epoch 00009: val_loss improved from 0.49913 to 0.47974, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 11/100\n",
      "100000/100000 [==============================] - 54s - loss: 0.4818 - acc: 0.8335 - val_loss: 0.4676 - val_acc: 0.8412\n",
      "Epoch 00010: val_loss improved from 0.47974 to 0.46760, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 12/100\n",
      "100000/100000 [==============================] - 54s - loss: 0.4688 - acc: 0.8383 - val_loss: 0.4626 - val_acc: 0.8440\n",
      "Epoch 00011: val_loss improved from 0.46760 to 0.46257, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 13/100\n",
      "100000/100000 [==============================] - 54s - loss: 0.4578 - acc: 0.8433 - val_loss: 0.4951 - val_acc: 0.8412\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 14/100\n",
      "100000/100000 [==============================] - 54s - loss: 0.4540 - acc: 0.8444 - val_loss: 0.4660 - val_acc: 0.8503\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 15/100\n",
      "100000/100000 [==============================] - 54s - loss: 0.4486 - acc: 0.8467 - val_loss: 0.4894 - val_acc: 0.8407\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 16/100\n",
      "100000/100000 [==============================] - 54s - loss: 0.4379 - acc: 0.8497 - val_loss: 0.4476 - val_acc: 0.8486\n",
      "Epoch 00015: val_loss improved from 0.46257 to 0.44760, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 17/100\n",
      "100000/100000 [==============================] - 54s - loss: 0.4333 - acc: 0.8515 - val_loss: 0.4832 - val_acc: 0.8516\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 18/100\n",
      "100000/100000 [==============================] - 54s - loss: 0.4276 - acc: 0.8529 - val_loss: 0.4804 - val_acc: 0.8502\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 19/100\n",
      "100000/100000 [==============================] - 54s - loss: 0.4289 - acc: 0.8531 - val_loss: 0.4509 - val_acc: 0.8566\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 20/100\n",
      "100000/100000 [==============================] - 54s - loss: 0.4229 - acc: 0.8565 - val_loss: 0.4490 - val_acc: 0.8563\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 21/100\n",
      "100000/100000 [==============================] - 54s - loss: 0.4180 - acc: 0.8562 - val_loss: 0.4726 - val_acc: 0.8502\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 22/100\n",
      "100000/100000 [==============================] - 54s - loss: 0.4151 - acc: 0.8590 - val_loss: 0.4749 - val_acc: 0.8468\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 23/100\n",
      "100000/100000 [==============================] - 54s - loss: 0.4109 - acc: 0.8596 - val_loss: 0.4801 - val_acc: 0.8553\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 24/100\n",
      "100000/100000 [==============================] - 54s - loss: 0.4080 - acc: 0.8608 - val_loss: 0.4700 - val_acc: 0.8495\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 25/100\n",
      "100000/100000 [==============================] - 54s - loss: 0.4045 - acc: 0.8625 - val_loss: 0.4621 - val_acc: 0.8540\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 26/100\n",
      "100000/100000 [==============================] - 54s - loss: 0.4070 - acc: 0.8625 - val_loss: 0.4793 - val_acc: 0.8510\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 27/100\n",
      "100000/100000 [==============================] - 54s - loss: 0.3998 - acc: 0.8641 - val_loss: 0.4788 - val_acc: 0.8545\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 00026: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff89f2ba6d8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3, border_mode='full',\n",
    "                        input_shape=(img_channels, img_rows, img_cols)))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='full'))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Convolution2D(128, 3, 3, border_mode='full'))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(Convolution2D(128, 3, 3))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', class_mode='categorical')\n",
    "    \n",
    "checkpointer = ModelCheckpoint(filepath='cifar10_cnn_keras_weights.hdf5', verbose=1, save_best_only=True)\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "\n",
    "model.fit(X_train, Y_train, \n",
    "          batch_size=batch_size, \n",
    "          nb_epoch=nb_epoch, \n",
    "          show_accuracy=True,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          callbacks=[checkpointer, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "layer_name_dict = {\n",
    "    'Dense': 'denseLayer',\n",
    "    'Dropout': 'dropoutLayer',\n",
    "    'Flatten': 'flattenLayer',\n",
    "    'Embedding': 'embeddingLayer',\n",
    "    'BatchNormalization': 'batchNormalizationLayer',\n",
    "    'LeakyReLU': 'leakyReLULayer',\n",
    "    'PReLU': 'parametricReLULayer',\n",
    "    'ParametricSoftplus': 'parametricSoftplusLayer',\n",
    "    'ThresholdedLinear': 'thresholdedLinearLayer',\n",
    "    'ThresholdedReLu': 'thresholdedReLuLayer',\n",
    "    'LSTM': 'rLSTMLayer',\n",
    "    'GRU': 'rGRULayer',\n",
    "    'JZS1': 'rJZS1Layer',\n",
    "    'JZS2': 'rJZS2Layer',\n",
    "    'JZS3': 'rJZS3Layer',\n",
    "    'Convolution2D': 'convolution2DLayer',\n",
    "    'MaxPooling2D': 'maxPooling2DLayer'\n",
    "}\n",
    "\n",
    "layer_params_dict = {\n",
    "    'Dense': ['weights', 'activation'],\n",
    "    'Dropout': ['p'],\n",
    "    'Flatten': [],\n",
    "    'Embedding': ['weights'],\n",
    "    'BatchNormalization': ['weights', 'epsilon'],\n",
    "    'LeakyReLU': ['alpha'],\n",
    "    'PReLU': ['weights'],\n",
    "    'ParametricSoftplus': ['weights'],\n",
    "    'ThresholdedLinear': ['theta'],\n",
    "    'ThresholdedReLu': ['theta'],\n",
    "    'LSTM': ['weights', 'activation', 'inner_activation', 'return_sequences'],\n",
    "    'GRU': ['weights', 'activation', 'inner_activation', 'return_sequences'],\n",
    "    'JZS1': ['weights', 'activation', 'inner_activation', 'return_sequences'],\n",
    "    'JZS2': ['weights', 'activation', 'inner_activation', 'return_sequences'],\n",
    "    'JZS3': ['weights', 'activation', 'inner_activation', 'return_sequences'],\n",
    "    'Convolution2D': ['weights', 'nb_filter', 'nb_row', 'nb_col', 'border_mode', 'subsample', 'activation'],\n",
    "    'MaxPooling2D': ['pool_size', 'stride', 'ignore_border']\n",
    "}\n",
    "\n",
    "layer_weights_dict = {\n",
    "    'Dense': ['W', 'b'],\n",
    "    'Embedding': ['E'],\n",
    "    'BatchNormalization': ['gamma', 'beta', 'mean', 'std'],\n",
    "    'PReLU': ['alphas'],\n",
    "    'ParametricSoftplus': ['alphas', 'betas'],\n",
    "    'LSTM': ['W_xi', 'W_hi', 'b_i', 'W_xc', 'W_hc', 'b_c', 'W_xf', 'W_hf', 'b_f', 'W_xo', 'W_ho', 'b_o'],\n",
    "    'GRU': ['W_xz', 'W_hz', 'b_z', 'W_xr', 'W_hr', 'b_r', 'W_xh', 'W_hh', 'b_h'],\n",
    "    'JZS1': ['W_xz', 'b_z', 'W_xr', 'W_hr', 'b_r', 'W_hh', 'b_h', 'Pmat'],\n",
    "    'JZS2': ['W_xz', 'W_hz', 'b_z', 'W_hr', 'b_r', 'W_xh', 'W_hh', 'b_h', 'Pmat'],\n",
    "    'JZS3': ['W_xz', 'W_hz', 'b_z', 'W_xr', 'W_hr', 'b_r', 'W_xh', 'W_hh', 'b_h'],\n",
    "    'Convolution2D': ['W', 'b']\n",
    "}\n",
    "\n",
    "def serialize(model_json_file, weights_hdf5_file, save_filepath, compress):\n",
    "    with open(model_json_file, 'r') as f:\n",
    "        model_metadata = json.load(f)\n",
    "    weights_file = h5py.File(weights_hdf5_file, 'r')\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    num_activation_layers = 0\n",
    "    for k, layer in enumerate(model_metadata['layers']):\n",
    "        if layer['name'] == 'Activation':\n",
    "            num_activation_layers += 1\n",
    "            prev_layer_name = model_metadata['layers'][k-1]['name']\n",
    "            idx_activation = layer_params_dict[prev_layer_name].index('activation')\n",
    "            layers[k-num_activation_layers]['parameters'][idx_activation] = layer['activation']\n",
    "            continue\n",
    "\n",
    "        layer_params = []\n",
    "\n",
    "        for param in layer_params_dict[layer['name']]:\n",
    "            if param == 'weights':\n",
    "                weights = {}\n",
    "                weight_names = layer_weights_dict[layer['name']]\n",
    "                for p, name in enumerate(weight_names):\n",
    "                    weights[name] = weights_file.get('layer_{}/param_{}'.format(k, p)).value.tolist()\n",
    "                layer_params.append(weights)\n",
    "            else:\n",
    "                layer_params.append(layer[param])\n",
    "\n",
    "        layers.append({\n",
    "            'layerName': layer_name_dict[layer['name']],\n",
    "            'parameters': layer_params\n",
    "        })\n",
    "\n",
    "    if compress:\n",
    "        with gzip.open(save_filepath, 'wb') as f:\n",
    "            f.write(json.dumps(layers).encode('utf8'))\n",
    "    else:\n",
    "        with open(save_filepath, 'w') as f:\n",
    "            json.dump(layers, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "model_metadata = json.loads(model.to_json())\n",
    "\n",
    "with open('cifar10_cnn_keras_model.json', 'w') as f:\n",
    "    json.dump(model_metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_mode': 'categorical',\n",
       " 'layers': [{'W_constraint': None,\n",
       "   'W_regularizer': None,\n",
       "   'activation': 'linear',\n",
       "   'activity_regularizer': None,\n",
       "   'b_constraint': None,\n",
       "   'b_regularizer': None,\n",
       "   'border_mode': 'full',\n",
       "   'init': 'glorot_uniform',\n",
       "   'input_shape': [3, 32, 32],\n",
       "   'name': 'Convolution2D',\n",
       "   'nb_col': 3,\n",
       "   'nb_filter': 32,\n",
       "   'nb_row': 3,\n",
       "   'subsample': [1, 1]},\n",
       "  {'alpha': 0.2, 'name': 'LeakyReLU'},\n",
       "  {'W_constraint': None,\n",
       "   'W_regularizer': None,\n",
       "   'activation': 'linear',\n",
       "   'activity_regularizer': None,\n",
       "   'b_constraint': None,\n",
       "   'b_regularizer': None,\n",
       "   'border_mode': 'valid',\n",
       "   'init': 'glorot_uniform',\n",
       "   'name': 'Convolution2D',\n",
       "   'nb_col': 3,\n",
       "   'nb_filter': 32,\n",
       "   'nb_row': 3,\n",
       "   'subsample': [1, 1]},\n",
       "  {'alpha': 0.2, 'name': 'LeakyReLU'},\n",
       "  {'ignore_border': True,\n",
       "   'name': 'MaxPooling2D',\n",
       "   'pool_size': [2, 2],\n",
       "   'stride': [2, 2]},\n",
       "  {'name': 'Dropout', 'p': 0.2},\n",
       "  {'W_constraint': None,\n",
       "   'W_regularizer': None,\n",
       "   'activation': 'linear',\n",
       "   'activity_regularizer': None,\n",
       "   'b_constraint': None,\n",
       "   'b_regularizer': None,\n",
       "   'border_mode': 'full',\n",
       "   'init': 'glorot_uniform',\n",
       "   'name': 'Convolution2D',\n",
       "   'nb_col': 3,\n",
       "   'nb_filter': 64,\n",
       "   'nb_row': 3,\n",
       "   'subsample': [1, 1]},\n",
       "  {'alpha': 0.2, 'name': 'LeakyReLU'},\n",
       "  {'W_constraint': None,\n",
       "   'W_regularizer': None,\n",
       "   'activation': 'linear',\n",
       "   'activity_regularizer': None,\n",
       "   'b_constraint': None,\n",
       "   'b_regularizer': None,\n",
       "   'border_mode': 'valid',\n",
       "   'init': 'glorot_uniform',\n",
       "   'name': 'Convolution2D',\n",
       "   'nb_col': 3,\n",
       "   'nb_filter': 64,\n",
       "   'nb_row': 3,\n",
       "   'subsample': [1, 1]},\n",
       "  {'alpha': 0.2, 'name': 'LeakyReLU'},\n",
       "  {'ignore_border': True,\n",
       "   'name': 'MaxPooling2D',\n",
       "   'pool_size': [2, 2],\n",
       "   'stride': [2, 2]},\n",
       "  {'name': 'Dropout', 'p': 0.3},\n",
       "  {'W_constraint': None,\n",
       "   'W_regularizer': None,\n",
       "   'activation': 'linear',\n",
       "   'activity_regularizer': None,\n",
       "   'b_constraint': None,\n",
       "   'b_regularizer': None,\n",
       "   'border_mode': 'full',\n",
       "   'init': 'glorot_uniform',\n",
       "   'name': 'Convolution2D',\n",
       "   'nb_col': 3,\n",
       "   'nb_filter': 128,\n",
       "   'nb_row': 3,\n",
       "   'subsample': [1, 1]},\n",
       "  {'alpha': 0.2, 'name': 'LeakyReLU'},\n",
       "  {'W_constraint': None,\n",
       "   'W_regularizer': None,\n",
       "   'activation': 'linear',\n",
       "   'activity_regularizer': None,\n",
       "   'b_constraint': None,\n",
       "   'b_regularizer': None,\n",
       "   'border_mode': 'valid',\n",
       "   'init': 'glorot_uniform',\n",
       "   'name': 'Convolution2D',\n",
       "   'nb_col': 3,\n",
       "   'nb_filter': 128,\n",
       "   'nb_row': 3,\n",
       "   'subsample': [1, 1]},\n",
       "  {'alpha': 0.2, 'name': 'LeakyReLU'},\n",
       "  {'ignore_border': True,\n",
       "   'name': 'MaxPooling2D',\n",
       "   'pool_size': [2, 2],\n",
       "   'stride': [2, 2]},\n",
       "  {'name': 'Dropout', 'p': 0.4},\n",
       "  {'name': 'Flatten'},\n",
       "  {'W_constraint': None,\n",
       "   'W_regularizer': None,\n",
       "   'activation': 'linear',\n",
       "   'activity_regularizer': None,\n",
       "   'b_constraint': None,\n",
       "   'b_regularizer': None,\n",
       "   'init': 'glorot_uniform',\n",
       "   'input_dim': None,\n",
       "   'name': 'Dense',\n",
       "   'output_dim': 512},\n",
       "  {'activation': 'relu', 'beta': 0.1, 'name': 'Activation', 'target': 0},\n",
       "  {'name': 'Dropout', 'p': 0.5},\n",
       "  {'W_constraint': None,\n",
       "   'W_regularizer': None,\n",
       "   'activation': 'linear',\n",
       "   'activity_regularizer': None,\n",
       "   'b_constraint': None,\n",
       "   'b_regularizer': None,\n",
       "   'init': 'glorot_uniform',\n",
       "   'input_dim': None,\n",
       "   'name': 'Dense',\n",
       "   'output_dim': 10},\n",
       "  {'activation': 'softmax', 'beta': 0.1, 'name': 'Activation', 'target': 0}],\n",
       " 'loss': 'categorical_crossentropy',\n",
       " 'name': 'Sequential',\n",
       " 'optimizer': {'beta_1': 0.9,\n",
       "  'beta_2': 0.999,\n",
       "  'epsilon': 1e-08,\n",
       "  'lr': 0.0010000000474974513,\n",
       "  'name': 'Adam'},\n",
       " 'theano_mode': None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "serialize('cifar10_cnn_keras_model.json', \n",
    "          'cifar10_cnn_keras_weights.hdf5', \n",
    "          'cifar10_cnn_model_params.json.gz', \n",
    "          True)\n",
    "serialize('cifar10_cnn_keras_model.json', \n",
    "          'cifar10_cnn_keras_weights.hdf5', \n",
    "          'cifar10_cnn_model_params.json', \n",
    "          False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "randidx = np.random.randint(0, X_test.shape[0], size=500)\n",
    "X_rand = X_test[randidx, :]\n",
    "y_rand = y_test[randidx]\n",
    "\n",
    "with gzip.open('sample_data.json.gz', 'wb') as f:\n",
    "    f.write(json.dumps({'data': X_rand.tolist(), 'labels': y_rand.tolist()}).encode('utf8'))\n",
    "with open('sample_data.json', 'w') as f:\n",
    "    json.dump({'data': X_rand.tolist(), 'labels': y_rand.tolist()}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 3.87 ms, total: 3.87 ms\n",
      "Wall time: 3.14 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  1.74755250e-08,   1.96856378e-10,   6.87712309e-05,\n",
       "          1.23311373e-04,   2.04585871e-04,   6.30733185e-03,\n",
       "          1.16306846e-05,   9.93284345e-01,   4.71561568e-10,\n",
       "          4.12435739e-08]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.predict(X_rand[0:1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
